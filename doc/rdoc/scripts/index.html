<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
    <meta name="Content-Type" content="text/html; charset=utf8" />
<title>Documentation by YARD 0.6.1</title>
<link rel="stylesheet" href="css/style.css" type="text/css" media="screen" charset="utf-8" />
<link rel="stylesheet" href="css/common.css" type="text/css" media="screen" charset="utf-8" />

<script type="text/javascript" charset="utf-8">
  relpath = '';
  if (relpath != '') relpath += '/';
</script>
<script type="text/javascript" charset="utf-8" src="js/jquery.js"></script>
<script type="text/javascript" charset="utf-8" src="js/app.js"></script>

  </head>
  <body>
    <script type="text/javascript" charset="utf-8">
      if (window.top.frames.main) document.body.className = 'frames';
    </script>
    
    <div id="header">
      <div id="menu">
  
    <a href="_index.html" title="Index">Index</a> &raquo; 
    <span class="title">File: README</span>
  
  
  <div class="noframes"><span class="title">(</span><a href="." target="_top">no frames</a><span class="title">)</span></div>
</div>

      <div id="search">
  <a id="class_list_link" href="#">Class List</a>
  <a id="method_list_link" href="#">Method List</a>
  <a id ="file_list_link" href="#">File List</a>
</div>

      <div class="clear"></div>
    </div>
    
    <iframe id="search_frame"></iframe>
    
    <div id="content"><div id='filecontents'><h1><a href="../index.html">Poodle</a> Crawling/Indexing</h1>

<p>The crawler is a separate script in <code>./Poodle/script/lib</code>.</p>

<p>Again the code is an example, I have used it on an intra-net but I would not trust it to crawl the web - If you do then I take no responsibility :-)</p>

<p>The script is in ruby and it has built in "help". I.e. at a command prompt (in the <code>./Poodle/script/lib</code> folder) type <code>crawler.rb -h</code> or <code>ruby crawler.rb -h</code> you should see something along the lines of:</p>

<pre class="code">    <span class='lparen token'>(</span><span class='web identifier id'>web</span><span class='rparen token'>)</span> <span class='crawler identifier id'>crawler</span> <span class='for for kw'>for</span> <span class='indexing identifier id'>indexing</span> <span class='content identifier id'>content</span> <span class='into identifier id'>into</span> <span class='Solr constant id'>Solr</span><span class='dot token'>.</span> <span class='To constant id'>To</span> <span class='use identifier id'>use</span> <span class='a identifier id'>a</span> <span class='proxy identifier id'>proxy</span><span class='comma token'>,</span> <span class='set identifier id'>set</span> <span class='http_proxy identifier id'>http_proxy</span><span class='assign token'>=</span><span class='http identifier id'>http</span><span class='symbol val'>:/</span><span class='regexp val'>/foo:1234

        -u, --url URL                    Initial URL to crawl
        -s, --solr URL                   URL to Solr
        -t, --title TEXT                 Strip TEXT from the title
        -l, --log NAME                   NAME of log file (else STDOUT)
        -a, --useragent NAME             User agent name
        -f, --from FROM                  From details
        -i, --ignore x,y,z               Ignore url's matching given patterns
        -w, --wait N                     Wait N seconds between each fetch
        -d, --depth D                    Max depth to crawl
        -e, --index                      Crawl AND index the content
        -q, --quiet                      Reduce log messages to informational only
            --yuk                        Horrible hack to fix poor CDATA termination, specific to a site - fix
        -h, --help                       Show this message
</span></pre>

<h2>Perform a test crawl</h2>

<p>Start up Solr (it should be configured, if not see <a href="file.INSTALLATION.html">installation</a>) by going into the solr example directory e.g. <code>/apache-solr-1.4.1/example/</code> and typing: <code>java -jar start.jar</code></p>

<p>Now run up the crawler against some content - I have provided a ".zip" of my blog and a small server to permit content to be accessed. For more information on setting this up, go <a href="../test-site/index.html">here</a></p>

<p>An example command line is as follows (as used to index a media-wiki site):</p>

<pre class="code">    <span class='ruby identifier id'>ruby</span> <span class='crawler identifier id'>crawler</span><span class='dot token'>.</span><span class='rb identifier id'>rb</span> <span class='minus op'>-</span><span class='l identifier id'>l</span> <span class='log identifier id'>log</span><span class='dot token'>.</span><span class='txt identifier id'>txt</span> <span class='minus op'>-</span><span class='t identifier id'>t</span> <span class='string val'>'- Wiki'</span> <span class='minus op'>-</span><span class='e identifier id'>e</span> <span class='minus op'>-</span><span class='a identifier id'>a</span> <span class='string val'>&quot;Search-Crawler/1.0&quot;</span> <span class='minus op'>-</span><span class='f identifier id'>f</span> <span class='string val'>&quot;foo@bar.com&quot;</span> <span class='minus op'>-</span><span class='u identifier id'>u</span> <span class='string val'>&quot;http://wiki/index.php?title=Main_Page&quot;</span> <span class='minus op'>-</span><span class='s identifier id'>s</span> <span class='string val'>&quot;http://localhost:8983/solr/&quot;</span> <span class='minus op'>-</span><span class='i identifier id'>i</span> <span class='Special constant id'>Special</span><span class='symbol val'>:,</span><span class='action identifier id'>action</span><span class='assign token'>=</span><span class='edit identifier id'>edit</span><span class='comma token'>,</span><span class='printable identifier id'>printable</span><span class='assign token'>=</span><span class='comma token'>,</span><span class='oldid identifier id'>oldid</span><span class='assign token'>=</span><span class='comma token'>,</span><span class='action identifier id'>action</span><span class='assign token'>=</span><span class='history identifier id'>history</span><span class='comma token'>,</span><span class='diff identifier id'>diff</span><span class='assign token'>=</span><span class='comma token'>,</span><span class='dot token'>.</span><span class='exe identifier id'>exe</span><span class='comma token'>,</span><span class='dot token'>.</span><span class='asp identifier id'>asp</span><span class='comma token'>,</span><span class='dot token'>.</span><span class='dot identifier id'>dot</span>
</pre>

<p>To crawl the example "test-site", you could use the following:</p>

<pre class="code"><span class='ruby identifier id'>ruby</span> <span class='crawler identifier id'>crawler</span><span class='dot token'>.</span><span class='rb identifier id'>rb</span> <span class='minus op'>-</span><span class='l identifier id'>l</span> <span class='mangled identifier id'>mangled</span><span class='dot token'>.</span><span class='log identifier id'>log</span> <span class='minus op'>-</span><span class='e identifier id'>e</span> <span class='minus op'>-</span><span class='a identifier id'>a</span> <span class='string val'>&quot;Search-Crawler/1.0&quot;</span> <span class='minus op'>-</span><span class='f identifier id'>f</span> <span class='string val'>&quot;foo@bar.com&quot;</span> <span class='minus op'>-</span><span class='u identifier id'>u</span> <span class='string val'>&quot;http://localhost:8080/blog/index.html&quot;</span> <span class='minus op'>-</span><span class='s identifier id'>s</span> <span class='string val'>&quot;http://localhost:8983/solr/&quot;</span> <span class='minus op'>-</span><span class='i identifier id'>i</span> <span class='category identifier id'>category</span><span class='comma token'>,</span><span class='tag identifier id'>tag</span>
</pre>

<p>You <em>need</em> to look at the logs and experiment with indexing as its highly likely you will need to fine tune rejection of URL's. Some site content especially blogs have a large amount of redundant links.</p>

<p>Repeated calls re-index content (updating existing and adding new, deleting old content isn't handled by the crawler, see "Refresh Crawled Content" below).</p>

<p>Once you have crawled some content you can use the Solr "GUI" to query, again I advise looking at the Solr documentation, if you have followed the default installation instructions the service should be at <a href="http://localhost:8983/solr/admin/">http://localhost:8983/solr/admin/</a>. If you crawled my sample blog you should be aware that the crawler needs tweaking to handle blog type content better (i.e. where a site has pages which link to many others), some terms to search for are 'lean' and 'ladder'. You should find better results using internal content.</p>

<p>You should run a cron job or Windows scheduled action to fire off indexing, say once a day.</p>

<h2>Refresh Crawled Content</h2>

<p>I have also written a script which checks the integrity of the Solr content, by checking stored URL's and deleting indexed items whose URL is "bad". This script should be run regularly, but less often than the indexers.</p>

<p>The script is called <code>purge.rb</code> and can be found in <code>./Poodle/script/lib</code>. Again it has built in help, if you type (in the <code>./Poodle/script/lib</code> folder) <code>purge.rb -h</code> or <code>ruby purge.rb -h</code> you should see something along the lines of:</p>

<pre class="code">    <span class='Purge constant id'>Purge</span> <span class='tool identifier id'>tool</span> <span class='for for kw'>for</span> <span class='cleaning identifier id'>cleaning</span> <span class='out identifier id'>out</span> <span class='invalid identifier id'>invalid</span> <span class='content identifier id'>content</span> <span class='from identifier id'>from</span> <span class='Solr constant id'>Solr</span> <span class='minus op'>-</span> <span class='Paired constant id'>Paired</span> <span class='with identifier id'>with</span> <span class='crawler identifier id'>crawler</span><span class='dot token'>.</span> <span class='To constant id'>To</span> <span class='use identifier id'>use</span> <span class='a identifier id'>a</span> <span class='proxy identifier id'>proxy</span><span class='comma token'>,</span> <span class='set identifier id'>set</span> <span class='http_proxy identifier id'>http_proxy</span><span class='assign token'>=</span><span class='http identifier id'>http</span><span class='symbol val'>:/</span><span class='regexp val'>/foo:1234

    -s, --solr URL                   URL to Solr
    -l, --log NAME                   NAME of log file (else STDOUT)
    -a, --useragent NAME             User agent name
    -f, --from FROM                  From details
    -w, --wait N                     Wait N seconds between each fetch
    -d, --delete                     Check AND delete the content
    -h, --help                       Show this message
</span></pre>

<p>It should be self-explanatory, an example usage would be:</p>

<pre class="code">    <span class='ruby identifier id'>ruby</span> <span class='dot token'>.</span>\<span class='lib identifier id'>lib</span>\<span class='purge identifier id'>purge</span><span class='dot token'>.</span><span class='rb identifier id'>rb</span> <span class='minus op'>-</span><span class='l identifier id'>l</span> <span class='purgelog identifier id'>purgelog</span><span class='dot token'>.</span><span class='txt identifier id'>txt</span> <span class='minus op'>-</span><span class='d identifier id'>d</span> <span class='minus op'>-</span><span class='a identifier id'>a</span> <span class='string val'>&quot;Purge/1.0&quot;</span> <span class='minus op'>-</span><span class='f identifier id'>f</span> <span class='string val'>&quot;foo@bar.com&quot;</span> <span class='minus op'>-</span><span class='s identifier id'>s</span> <span class='string val'>&quot;http://localhost:8983/solr&quot;</span>
</pre>

<h2>Proxies/Windows NTLM etc.</h2>

<p>If you have problems with IIS and windows, then look at <a href="http://sourceforge.net/projects/ntlmaps/">NTLMAPS</a>, i.e. point the crawler at a the ntlmaps proxy and let it rip, works fine for me.</p>

<p>I have also had success using <a href="http://rubyforge.org/projects/rubysspi/">rubysspi</a>. Both the crawler and purge tools check for this and if it's available will use it. When using a proxy you will need to set the environment variable <code>http_proxy</code>. E.g. <code>http_proxy=http://foo:1234</code></p>

<h2>Issues</h2>

<p>The crawler is still very much WIP and I expect it to be cleaner by version 1.0. It's grown organically and to be honest needs tidying.</p>

<p>Major limitations are no support for <code>robots.txt</code>, it's loop detection is almost non-existent - So it could quite easily wander off forever! It also lacks support for authentication and could do with indicating the content types it accepts. Lastly, it re-crawls and indexes all content and uses curl to stream with - Not optimal. Extending the crawler to handle these omissions isn't hard, I just didn't need to (crawling an intranet lowers or removes the need to implement these features).</p>

<p>If you find this code of use, you could help me write some crawlers for different content, shared files/folders, MS outlook/exchange content, Trac, the list is quite long.</p>

<h2>What Next?</h2>

<p>Now you have some content crawled/indexed, try running up the web front end to <a href="../poodle/index.html">search the content</a></p></div></div>
    
    <div id="footer">
  Generated on Tue Oct 19 13:34:28 2010 by 
  <a href="http://yardoc.org" title="Yay! A Ruby Documentation Tool" target="_parent">yard</a>
  0.6.1 (ruby-1.8.7).
</div>

  </body>
</html>